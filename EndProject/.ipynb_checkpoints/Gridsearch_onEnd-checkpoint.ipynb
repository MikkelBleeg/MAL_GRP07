{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            r=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                t = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(r)>0:\n",
    "                    r += ','\n",
    "                r += f'{key}={t}{value}{t}'  \n",
    "            return r            \n",
    "        try:\n",
    "            p = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + p + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    Xx_test, yy_test=np.array(X_test),np.array(y_test)\n",
    "    assert Xx_test.shape[0]==yy_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Load time=15.2\n"
     ]
    }
   ],
   "source": [
    "#TODO load the dataset:\n",
    "import sys,os\n",
    "from time import time\n",
    "sys.path.append(os.path.expanduser('ProjectFunctions'))\n",
    "from ProjectFunctions import LoadShapes as LS\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "X, y = LS.getShapes()\n",
    "\n",
    "t = time()-start\n",
    "print(f\"OK, Load time={t:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X:  (14970, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X_shape = X.shape\n",
    "print(\"The shape of X: \", X_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape of the reshaped X:  (14970, 40000)\n",
      "The shape of X ready to be inputed in the CNN:  (40000,)\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "X_r = X.reshape(14970, 200*200)\n",
    "\n",
    "X_rShape = X_r.shape\n",
    "print(\"The new shape of the reshaped X: \", X_rShape)\n",
    "\n",
    "input_X_rShape = X_rShape[1:]\n",
    "print(\"The shape of X ready to be inputed in the CNN: \", input_X_rShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing data for neural networks.\n"
     ]
    }
   ],
   "source": [
    "X_neuralReady=np.array(X_r)/255\n",
    "\n",
    "print(\"Done preparing data for neural networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting train and test data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Done splitting train and test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting train and test data ready for neural networks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xnr_train, Xnr_test, ynr_train, ynr_test = train_test_split(X_neuralReady, y, test_size=test_size, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Done splitting train and test data ready for neural networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnr_train, Xnr_test, ynr_train, ynr_test = Xnr_train[:1000], Xnr_test[:1000], ynr_train[:1000], ynr_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[0.1, 1, 10]\n",
    "}\n",
    "\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(Xnr_train, ynr_train)\n",
    "t = time()-start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 256.93 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'rbf'}\n",
      "\tbest 'f1_micro' score=1.0\n",
      "\tbest index=3\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.998 (+/-0.005) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.997 (+/-0.012) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.998 (+/-0.005) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 1.000 (+/-0.000) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.998 (+/-0.005) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 1.000 (+/-0.000) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Circle       1.00      1.00      1.00       218\n",
      "      Square       1.00      1.00      1.00       273\n",
      "        Star       1.00      1.00      1.00       269\n",
      "    Triangle       1.00      1.00      1.00       240\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "best: dat=N/A, score=1.00000, model=SVC(C=1,kernel='rbf')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , Xnr_test, ynr_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 6 is smaller than n_iter=20. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 238.46 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'kernel': 'rbf', 'C': 1}\n",
      "\tbest 'f1_micro' score=1.0\n",
      "\tbest index=3\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.998 (+/-0.005) for {'kernel': 'linear', 'C': 0.1}\n",
      "\t[ 1]: 0.997 (+/-0.012) for {'kernel': 'rbf', 'C': 0.1}\n",
      "\t[ 2]: 0.998 (+/-0.005) for {'kernel': 'linear', 'C': 1}\n",
      "\t[ 3]: 1.000 (+/-0.000) for {'kernel': 'rbf', 'C': 1}\n",
      "\t[ 4]: 0.998 (+/-0.005) for {'kernel': 'linear', 'C': 10}\n",
      "\t[ 5]: 1.000 (+/-0.000) for {'kernel': 'rbf', 'C': 10}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Circle       1.00      1.00      1.00       218\n",
      "      Square       1.00      1.00      1.00       273\n",
      "        Star       1.00      1.00      1.00       269\n",
      "    Triangle       1.00      1.00      1.00       240\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "best: dat=N/A, score=1.00000, model=SVC(C=1,kernel='rbf')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "# Run RandomizedSearchCV for the model\n",
    "start = time()\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, random_state=42, n_iter=20, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "random_tuned.fit(Xnr_train, ynr_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(random_tuned , Xnr_test, ynr_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion on optimization\n",
    "\n",
    "For optimizing of chosen dataset, GridSearchCV and RandomSearchCV was used. Both optimization method originates from scikit-learn. To find the best optimization, the search methods was used with different combinations of tuning parameters, implemented in a parameter_grid. the search methods will try to optimize the models using the parameters given, which then results in scores to given parameters. The scores indicates which tuning parameters are best for optimizing. The best scoring parameters is known as the best parameters for the given dataset.\n",
    "\n",
    "When comparing the GridSearch and RandomSearch, the RandomSearch is slightly faster, which is expected as it doesn't try with all combinations. As seen in the results above, the search found the best estimator for optimizing by trying combinations with the given tuning parameters. The various combinations saw scores of 0.997 and 0.998. \n",
    "With the best estimator for optimization a score of 1, which is a perfect score,was achieved. The tuning parameter for the best estimator is found to be: Kernel=rbf and C-gamma=1.\n",
    "\n",
    "But with a score of 1, it could be questionable whether the system is overfitted.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
