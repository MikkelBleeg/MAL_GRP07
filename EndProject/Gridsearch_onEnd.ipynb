{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For GPU server\n",
    "# !git clone https://cfrigaard@bitbucket.org/cfrigaard/itmal\n",
    "\n",
    "# import sys,os\n",
    "# sys.path.append(os.path.expanduser('~/itmal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "# config.gpu_options.allow_growth=True\n",
    "# set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from libitmal import dataloaders as itmaldataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            r=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                t = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(r)>0:\n",
    "                    r += ','\n",
    "                r += f'{key}={t}{value}{t}'  \n",
    "            return r            \n",
    "        try:\n",
    "            p = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + p + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    Xx_test, yy_test=np.array(X_test),np.array(y_test)\n",
    "    assert Xx_test.shape[0]==yy_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "\n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Load time=39.9\n"
     ]
    }
   ],
   "source": [
    "#TODO load the dataset:\n",
    "import sys,os\n",
    "from time import time\n",
    "sys.path.append(os.path.expanduser('ProjectFunctions'))\n",
    "from ProjectFunctions import LoadShapes as LS\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "X, y = LS.getShapes()\n",
    "\n",
    "t = time()-start\n",
    "print(f\"OK, Load time={t:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X:  (14970, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X_shape = X.shape\n",
    "print(\"The shape of X: \", X_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new shape of the reshaped X:  (14970, 40000)\n",
      "The shape of X ready to be inputed in the CNN:  (40000,)\n"
     ]
    }
   ],
   "source": [
    "#split\n",
    "X_r = X.reshape(14970, 200*200)\n",
    "\n",
    "X_rShape = X_r.shape\n",
    "print(\"The new shape of the reshaped X: \", X_rShape)\n",
    "\n",
    "input_X_rShape = X_rShape[1:]\n",
    "print(\"The shape of X ready to be inputed in the CNN: \", input_X_rShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing data for neural networks.\n"
     ]
    }
   ],
   "source": [
    "X_neuralReady=np.array(X_r)/255\n",
    "\n",
    "print(\"Done preparing data for neural networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting train and test data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size=0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Done splitting train and test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting train and test data ready for neural networks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xnr_train, Xnr_test, ynr_train, ynr_test = train_test_split(X_neuralReady, y, test_size=test_size, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"Done splitting train and test data ready for neural networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess split: 0.00 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "Xnr_train, Xnr_test, ynr_train, ynr_test = Xnr_train[:1000], Xnr_test[:1000], ynr_train[:1000], ynr_test[:1000]\n",
    "# Xnr_train_split, Xnr_test_split = preprocessing.scale(Xnr_train), preprocessing.scale(Xnr_test)\n",
    "# ynr_train_split, ynr_test_split = ynr_train, ynr_test\n",
    "t = time()-start\n",
    "print(f\"preprocess split: {t:0.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spending searching: 0.0\n"
     ]
    }
   ],
   "source": [
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 3]\n",
    "}\n",
    "\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(Xnr_train, ynr_train)\n",
    "t = time()-start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 259.31 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'rbf'}\n",
      "\tbest 'f1_micro' score=1.0\n",
      "\tbest index=1\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.998 (+/-0.005) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 1]: 1.000 (+/-0.000) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.998 (+/-0.005) for {'C': 3, 'kernel': 'linear'}\n",
      "\t[ 3]: 1.000 (+/-0.000) for {'C': 3, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Circle       1.00      1.00      1.00       218\n",
      "      Square       1.00      1.00      1.00       273\n",
      "        Star       1.00      1.00      1.00       269\n",
      "    Triangle       1.00      1.00      1.00       240\n",
      "\n",
      "    accuracy                           1.00      1000\n",
      "   macro avg       1.00      1.00      1.00      1000\n",
      "weighted avg       1.00      1.00      1.00      1000\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "best: dat=N/A, score=1.00000, model=SVC(C=1,kernel='rbf')\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , Xnr_test, ynr_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 3]\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "# Run RandomizedSearchCV for the model\n",
    "start = time()\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, random_state=42, n_iter=20, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "random_tuned.fit(Xnr_train, ynr_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(random_tuned , Xnr_test, ynr_test, t)\n",
    "print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
