{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0318| CEF, initial.\n",
    "2018-0321| CEF, synced with MLP moon exercise.\n",
    "2018-0323| CEF, minor updated and spell checked.\n",
    "2019-0930| CEF, updated for ITMAL E19.\n",
    "\n",
    "\n",
    "## Keras Multi-Layer Perceptrons (MLP's) on MNIST-data\n",
    "\n",
    "\n",
    "### Qa Using a Keras MLP on the MNIST-data\n",
    "\n",
    "Now, make a Keras `Sequential` model and fit it to the MNIST data, re-using as much of the code form the `mlp_moon.ipynb` as you can.\n",
    "\n",
    "Then try to change the number of hidden layers and the neurons in each layer, looking for increases in test accuracy via ``score``. \n",
    "\n",
    "Publish your best score for your model in Blackboard, see link under L06. We use categorical accuracy for score---eventhough a $F_1$ score could say more. Publish you result like\n",
    "```\n",
    "   ITMALGrpXY: score=0.76, a 10-20-30-20-10 MLP, takes looong to train\n",
    "```\n",
    "or similar\n",
    "\n",
    "\n",
    "NOTE: you probably need to scale/normalize the MNIST data before a fit, and no 2D-decision boundaries can be drawn from the 784-dimension MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load MNIST data.\n",
    "def MNIST_GetDataSet():\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', version=1 ,return_X_y=True)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Load time=59.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Loading The MNIST dataset\n",
    "\n",
    "#Fetching MNIST data from skillshare with own function.\n",
    "start = time()\n",
    "X,y = MNIST_GetDataSet()\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, Load time={t:0.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the range of the pixel value from 0 to 255 to 0 to 1. Because the neurons and activation functions are operating in the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the range of the pixel value to 0 to 1. \n",
    "X1=X/255\n",
    "\n",
    "# print(X[0])\n",
    "# print(X1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Split time=2.4\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.22352941 0.6        0.99607843 0.99607843\n",
      " 0.58039216 0.05098039 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.20784314 0.69411765\n",
      " 0.81176471 0.99215686 0.99215686 0.99215686 0.99215686 0.69019608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.9254902  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.88627451 0.10196078 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38823529 0.89803922\n",
      " 0.99607843 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.69803922 0.01176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.10588235 0.48627451 0.90588235 0.99215686 0.99607843 0.99215686\n",
      " 0.90980392 0.7372549  0.34509804 0.68235294 0.88235294 0.99215686\n",
      " 0.99215686 0.12941176 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.42745098 0.88235294 0.99215686\n",
      " 0.99215686 0.99215686 0.99607843 0.74901961 0.20392157 0.\n",
      " 0.         0.         0.47843137 0.99215686 0.99215686 0.12941176\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.41176471 0.97647059 0.99215686 0.99215686 0.99215686 0.97647059\n",
      " 0.2745098  0.02745098 0.         0.         0.         0.\n",
      " 0.47843137 0.99215686 0.99215686 0.12941176 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32156863 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.73333333 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.         0.55294118 0.99215686\n",
      " 0.99215686 0.12941176 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.84313725 0.99215686 0.99215686 0.99215686 0.54117647\n",
      " 0.06666667 0.         0.         0.         0.         0.\n",
      " 0.         0.39215686 0.97254902 0.99215686 0.90196078 0.09019608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.71764706 0.98823529\n",
      " 0.99215686 0.99215686 0.38823529 0.05098039 0.         0.\n",
      " 0.         0.         0.         0.         0.09803922 0.8745098\n",
      " 0.99215686 0.99215686 0.50196078 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.85098039 0.99607843 0.99607843 0.74509804\n",
      " 0.03921569 0.         0.         0.         0.         0.\n",
      " 0.         0.05098039 0.65882353 0.99607843 0.99607843 0.82352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37647059\n",
      " 0.99215686 0.99215686 0.84313725 0.06666667 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.50196078\n",
      " 0.99215686 0.99215686 0.99215686 0.70588235 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52156863 0.99215686 0.99215686\n",
      " 0.60392157 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.49803922 0.98823529 0.99215686 0.99215686\n",
      " 0.78823529 0.06666667 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07058824 0.83529412 0.99215686 0.99215686 0.34509804 0.\n",
      " 0.         0.         0.         0.         0.14509804 0.82352941\n",
      " 0.98823529 0.99215686 0.99215686 0.96470588 0.28235294 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13333333 0.99215686\n",
      " 0.99215686 0.99215686 0.34509804 0.         0.         0.13333333\n",
      " 0.35294118 0.65098039 1.         0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.36470588 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.13333333 0.99215686 0.99215686 0.99215686\n",
      " 0.76078431 0.34901961 0.55294118 0.90980392 0.99215686 0.99215686\n",
      " 0.99607843 0.99215686 0.99215686 0.96470588 0.54901961 0.04705882\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01176471 0.69803922 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.99607843 0.99215686\n",
      " 0.97647059 0.49019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.21568627\n",
      " 0.96470588 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.89803922 0.51764706 0.23529412 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38431373 0.98039216\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.90196078 0.50196078\n",
      " 0.11372549 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901961 0.65490196 0.99215686\n",
      " 0.59607843 0.48627451 0.09019608 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting Data\n",
    "# Make data\n",
    "start = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary  = to_categorical(y_test)\n",
    "t = time()-start\n",
    "\n",
    "# Controls the dimention of the train/test arrays.\n",
    "assert y.ndim==1                     \n",
    "assert y_train_binary.ndim==2       \n",
    "assert y_test_binary.ndim ==2       \n",
    "\n",
    "print(f\"OK, Split time={t:0.1f}\")\n",
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#from sklearn import datasets\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build Keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, units=2000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(1000,activation='tanh'))\n",
    "model.add(Dense(500,activation='tanh'))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = SGD(lr=0.01)\n",
    "#optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# Train\n",
    "VERBOSE     = 0\n",
    "EPOCHS      = 35\n",
    "\n",
    "start = time()\n",
    "history = model.fit(X_train, y_train_binary, validation_data=(X_test, y_test_binary), epochs=EPOCHS, verbose=VERBOSE)\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, training time={t:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(history.history)\n",
    "score = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "\n",
    "print(f\"Training time: {t:0.1f} sec\")\n",
    "print(f\"Test loss:     {score[0]}\") # loss is score 0 by definition?\n",
    "print(f\"Test accuracy: {score[1]}\")\n",
    "print(f\"All scores in history: {score}\")\n",
    "\n",
    "N=4\n",
    "FX=60\n",
    "FY=4\n",
    "A=0.4\n",
    "S=4\n",
    "\n",
    "# Plot org data\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 1)\n",
    "colors = ['steelblue' if label == 1 else 'darkred' for label in y]\n",
    "plt.scatter(X[:,0], X[:,1], color=colors, alpha=.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 2)\n",
    "plt.plot(history.history[\"loss\"]    , \"b--x\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_loss\"], \"g-s\" , markerfacecolor=(0, 1, 0, A), markersize=S)\n",
    "plt.legend(loc=\"best\", labels=(\"loss(train)\",\"loss(val)\"))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot all metrics + loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 3)\n",
    "plt.plot(history.history[\"mean_squared_error\"],      \"r:x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_squared_error\"],  \"r-x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"mean_absolute_error\"],     \"b:o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_absolute_error\"], \"b-o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"best\", labels=(\"mean_squared_error(train)\",  \"mean_squared_error(val)\", \n",
    "                               \"mean_absolute_error(train)\", \"mean_absolute_error(val)\", \n",
    "                               \"loss(categorical_crossentropy,train)\", \"loss(categorical_crossentropy,val)\"))\n",
    "plt.title(\"Error-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 4)\n",
    "plt.plot(history.history[\"categorical_accuracy\"],     \"m-x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_categorical_accuracy\"], \"m:x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "ax.set_ylim([0,1])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"lower right\", labels=(\"categorical_accuracy\",))\n",
    "plt.title(\"Accuracy-vs-epoch plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1:\n",
    "# Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=20, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# #optimizer = SGD(lr=0.1)\n",
    "# optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 50\n",
    "\n",
    "# Training time: 527.9 sec\n",
    "# Test loss:     0.22195826129615306\n",
    "# Test accuracy: 0.943\n",
    "# All scores in history: [0.22195826129615306, 0.943, 0.009008236024033977, 0.014796660615840838]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # TEST 2:\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=20, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 50\n",
    "\n",
    "# Training time: 232.0 sec\n",
    "# Test loss:     0.17488471111797152\n",
    "# Test accuracy: 0.9501428571428572\n",
    "# All scores in history: [0.17488471111797152, 0.9501428571428572, 0.0076049974263268745, 0.016699722670373462]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TEST 3\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=200, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(20,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 35\n",
    "\n",
    "# Training time: 279.0 sec\n",
    "# Test loss:     0.10130165741024982\n",
    "# Test accuracy: 0.9695714285714285\n",
    "# All scores in history: [0.10130165741024982, 0.9695714285714285, 0.0046379666870295, 0.009067855959669465]\n",
    "\n",
    "# # TEST 4\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=1000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(600,activation='tanh'))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 10\n",
    "\n",
    "# Training time: 227.6 sec\n",
    "# Test loss:     0.13001273374472347\n",
    "# Test accuracy: 0.9621428571428572\n",
    "# All scores in history: [0.13001273374472347, 0.9621428571428572, 0.005777994256799242, 0.0136300406851584]\n",
    "\n",
    "\n",
    "\n",
    "# #TEST 5\n",
    "# Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=1000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(600,activation='tanh'))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 35\n",
    "\n",
    "# Training time: 758.5 sec\n",
    "# Test loss:     0.08972824973967813\n",
    "# Test accuracy: 0.9724285714285714\n",
    "# All scores in history: [0.08972824973967813, 0.9724285714285714, 0.00416490629911104, 0.008350563158591588]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
