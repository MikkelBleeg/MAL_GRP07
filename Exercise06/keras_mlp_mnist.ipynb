{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0318| CEF, initial.\n",
    "2018-0321| CEF, synced with MLP moon exercise.\n",
    "2018-0323| CEF, minor updated and spell checked.\n",
    "2019-0930| CEF, updated for ITMAL E19.\n",
    "\n",
    "\n",
    "## Keras Multi-Layer Perceptrons (MLP's) on MNIST-data\n",
    "\n",
    "\n",
    "### Qa Using a Keras MLP on the MNIST-data\n",
    "\n",
    "Now, make a Keras `Sequential` model and fit it to the MNIST data, re-using as much of the code form the `mlp_moon.ipynb` as you can.\n",
    "\n",
    "Then try to change the number of hidden layers and the neurons in each layer, looking for increases in test accuracy via ``score``. \n",
    "\n",
    "Publish your best score for your model in Blackboard, see link under L06. We use categorical accuracy for score---eventhough a $F_1$ score could say more. Publish you result like\n",
    "```\n",
    "   ITMALGrpXY: score=0.76, a 10-20-30-20-10 MLP, takes looong to train\n",
    "```\n",
    "or similar\n",
    "\n",
    "\n",
    "NOTE: you probably need to scale/normalize the MNIST data before a fit, and no 2D-decision boundaries can be drawn from the 784-dimension MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load MNIST data.\n",
    "def MNIST_GetDataSet():\n",
    "    # Load data from https://www.openml.org/d/554\n",
    "    X, y = fetch_openml('mnist_784', version=1 ,return_X_y=True)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Load time=54.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Loading The MNIST dataset\n",
    "\n",
    "#Fetching MNIST data from skillshare with own function.\n",
    "start = time()\n",
    "X,y = MNIST_GetDataSet()\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, Load time={t:0.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the range of the pixel value from 0 to 255 to 0 to 1. Because the neurons and activation functions are operating in the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "  18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      " 253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      " 253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      " 198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "  11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      " 225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      " 240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      " 229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      " 253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      " 253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "  80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "\n",
    "# Changing the range of the pixel value to 0 to 1. \n",
    "X1=X/255\n",
    "\n",
    "print(X[0])\n",
    "print(X1[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, Split time=1.5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.22352941 0.6        0.99607843 0.99607843\n",
      " 0.58039216 0.05098039 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.20784314 0.69411765\n",
      " 0.81176471 0.99215686 0.99215686 0.99215686 0.99215686 0.69019608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.9254902  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.88627451 0.10196078 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38823529 0.89803922\n",
      " 0.99607843 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.69803922 0.01176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.10588235 0.48627451 0.90588235 0.99215686 0.99607843 0.99215686\n",
      " 0.90980392 0.7372549  0.34509804 0.68235294 0.88235294 0.99215686\n",
      " 0.99215686 0.12941176 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.42745098 0.88235294 0.99215686\n",
      " 0.99215686 0.99215686 0.99607843 0.74901961 0.20392157 0.\n",
      " 0.         0.         0.47843137 0.99215686 0.99215686 0.12941176\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.41176471 0.97647059 0.99215686 0.99215686 0.99215686 0.97647059\n",
      " 0.2745098  0.02745098 0.         0.         0.         0.\n",
      " 0.47843137 0.99215686 0.99215686 0.12941176 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.32156863 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.73333333 0.21960784 0.         0.\n",
      " 0.         0.         0.         0.         0.55294118 0.99215686\n",
      " 0.99215686 0.12941176 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.84313725 0.99215686 0.99215686 0.99215686 0.54117647\n",
      " 0.06666667 0.         0.         0.         0.         0.\n",
      " 0.         0.39215686 0.97254902 0.99215686 0.90196078 0.09019608\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.71764706 0.98823529\n",
      " 0.99215686 0.99215686 0.38823529 0.05098039 0.         0.\n",
      " 0.         0.         0.         0.         0.09803922 0.8745098\n",
      " 0.99215686 0.99215686 0.50196078 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.85098039 0.99607843 0.99607843 0.74509804\n",
      " 0.03921569 0.         0.         0.         0.         0.\n",
      " 0.         0.05098039 0.65882353 0.99607843 0.99607843 0.82352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.37647059\n",
      " 0.99215686 0.99215686 0.84313725 0.06666667 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.50196078\n",
      " 0.99215686 0.99215686 0.99215686 0.70588235 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.52156863 0.99215686 0.99215686\n",
      " 0.60392157 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.49803922 0.98823529 0.99215686 0.99215686\n",
      " 0.78823529 0.06666667 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07058824 0.83529412 0.99215686 0.99215686 0.34509804 0.\n",
      " 0.         0.         0.         0.         0.14509804 0.82352941\n",
      " 0.98823529 0.99215686 0.99215686 0.96470588 0.28235294 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13333333 0.99215686\n",
      " 0.99215686 0.99215686 0.34509804 0.         0.         0.13333333\n",
      " 0.35294118 0.65098039 1.         0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.36470588 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.13333333 0.99215686 0.99215686 0.99215686\n",
      " 0.76078431 0.34901961 0.55294118 0.90980392 0.99215686 0.99215686\n",
      " 0.99607843 0.99215686 0.99215686 0.96470588 0.54901961 0.04705882\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.01176471 0.69803922 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.99607843 0.99215686\n",
      " 0.97647059 0.49019608 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.21568627\n",
      " 0.96470588 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.89803922 0.51764706 0.23529412 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.38431373 0.98039216\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.90196078 0.50196078\n",
      " 0.11372549 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34901961 0.65490196 0.99215686\n",
      " 0.59607843 0.48627451 0.09019608 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting Data\n",
    "# Make data\n",
    "start = time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary  = to_categorical(y_test)\n",
    "t = time()-start\n",
    "\n",
    "# Controls the dimention of the train/test arrays.\n",
    "assert y.ndim==1                     \n",
    "assert y_train_binary.ndim==2       \n",
    "assert y_test_binary.ndim ==2       \n",
    "\n",
    "print(f\"OK, Split time={t:0.1f}\")\n",
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.  51. 159. 253. 159.  50.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  48. 238. 252. 252. 252. 237.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  54.\n",
      " 227. 253. 252. 239. 233. 252.  57.   6.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  10.  60. 224.\n",
      " 252. 253. 252. 202.  84. 252. 253. 122.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 163. 252. 252.\n",
      " 252. 253. 252. 252.  96. 189. 253. 167.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  51. 238. 253. 253.\n",
      " 190. 114. 253. 228.  47.  79. 255. 168.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.  48. 238. 252. 252. 179.\n",
      "  12.  75. 121.  21.   0.   0. 253. 243.  50.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  38. 165. 253. 233. 208.  84.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 165.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   7. 178. 252. 240.  71.  19.  28.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  57. 252. 252.  63.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 195.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 198. 253. 190.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 255. 253. 196.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  76. 246. 252. 112.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 253. 252. 148.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 230.  25.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   7. 135. 253. 186.  12.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 223.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   7. 131. 252. 225.  71.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 145.   0.   0.   0.   0.   0.\n",
      "   0.   0.  48. 165. 252. 173.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  86. 253. 225.   0.   0.   0.   0.   0.\n",
      "   0. 114. 238. 253. 162.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 249. 146.  48.  29.  85. 178.\n",
      " 225. 253. 223. 167.  56.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  85. 252. 252. 252. 229. 215. 252. 252.\n",
      " 252. 196. 130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  28. 199. 252. 252. 253. 252. 252. 233.\n",
      " 145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  25. 128. 252. 253. 252. 141.  37.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X[1])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Trung\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "#from sklearn import datasets\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build Keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, units=2000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(1000,activation='tanh'))\n",
    "model.add(Dense(500,activation='tanh'))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = SGD(lr=0.01)\n",
    "#optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# Train\n",
    "VERBOSE     = 0\n",
    "EPOCHS      = 35\n",
    "\n",
    "start = time()\n",
    "history = model.fit(X_train, y_train_binary, validation_data=(X_test, y_test_binary), epochs=EPOCHS, verbose=VERBOSE)\n",
    "t = time()-start\n",
    "\n",
    "print(f\"OK, training time={t:0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(history.history)\n",
    "score = model.evaluate(X_test, y_test_binary, verbose=0)\n",
    "\n",
    "print(f\"Training time: {t:0.1f} sec\")\n",
    "print(f\"Test loss:     {score[0]}\") # loss is score 0 by definition?\n",
    "print(f\"Test accuracy: {score[1]}\")\n",
    "print(f\"All scores in history: {score}\")\n",
    "\n",
    "N=4\n",
    "FX=60\n",
    "FY=4\n",
    "A=0.4\n",
    "S=4\n",
    "\n",
    "# Plot org data\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 1)\n",
    "colors = ['steelblue' if label == 1 else 'darkred' for label in y]\n",
    "plt.scatter(X[:,0], X[:,1], color=colors, alpha=.5)\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 2)\n",
    "plt.plot(history.history[\"loss\"]    , \"b--x\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_loss\"], \"g-s\" , markerfacecolor=(0, 1, 0, A), markersize=S)\n",
    "plt.legend(loc=\"best\", labels=(\"loss(train)\",\"loss(val)\"))\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot all metrics + loss\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 3)\n",
    "plt.plot(history.history[\"mean_squared_error\"],      \"r:x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_squared_error\"],  \"r-x\", markerfacecolor=(1, 0, 0, A), markersize=S)\n",
    "plt.plot(history.history[\"mean_absolute_error\"],     \"b:o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_mean_absolute_error\"], \"b-o\", markerfacecolor=(0, 0, 1, A), markersize=S)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"best\", labels=(\"mean_squared_error(train)\",  \"mean_squared_error(val)\", \n",
    "                               \"mean_absolute_error(train)\", \"mean_absolute_error(val)\", \n",
    "                               \"loss(categorical_crossentropy,train)\", \"loss(categorical_crossentropy,val)\"))\n",
    "plt.title(\"Error-vs-epoch plot\")\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(FX, FY))\n",
    "ax = plt.subplot(1, N, 4)\n",
    "plt.plot(history.history[\"categorical_accuracy\"],     \"m-x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "plt.plot(history.history[\"val_categorical_accuracy\"], \"m:x\", markerfacecolor=(1, 0, 1, A), markersize=S)\n",
    "ax.set_ylim([0,1])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlim((0, EPOCHS))\n",
    "plt.legend(loc=\"lower right\", labels=(\"categorical_accuracy\",))\n",
    "plt.title(\"Accuracy-vs-epoch plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1:\n",
    "# Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=20, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# #optimizer = SGD(lr=0.1)\n",
    "# optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 50\n",
    "\n",
    "# Training time: 527.9 sec\n",
    "# Test loss:     0.22195826129615306\n",
    "# Test accuracy: 0.943\n",
    "# All scores in history: [0.22195826129615306, 0.943, 0.009008236024033977, 0.014796660615840838]\n",
    "    \n",
    "    \n",
    "# # TEST 2:\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=20, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 50\n",
    "\n",
    "# Training time: 232.0 sec\n",
    "# Test loss:     0.17488471111797152\n",
    "# Test accuracy: 0.9501428571428572\n",
    "# All scores in history: [0.17488471111797152, 0.9501428571428572, 0.0076049974263268745, 0.016699722670373462]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TEST 3\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=200, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(20,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 35\n",
    "\n",
    "# Training time: 279.0 sec\n",
    "# Test loss:     0.10130165741024982\n",
    "# Test accuracy: 0.9695714285714285\n",
    "# All scores in history: [0.10130165741024982, 0.9695714285714285, 0.0046379666870295, 0.009067855959669465]\n",
    "\n",
    "# # TEST 4\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=1000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(600,activation='tanh'))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 10\n",
    "\n",
    "# Training time: 227.6 sec\n",
    "# Test loss:     0.13001273374472347\n",
    "# Test accuracy: 0.9621428571428572\n",
    "# All scores in history: [0.13001273374472347, 0.9621428571428572, 0.005777994256799242, 0.0136300406851584]\n",
    "\n",
    "\n",
    "\n",
    "# #TEST 5\n",
    "# Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=1000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(600,activation='tanh'))\n",
    "# model.add(Dense(100,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 35\n",
    "\n",
    "# Training time: 758.5 sec\n",
    "# Test loss:     0.08972824973967813\n",
    "# Test accuracy: 0.9724285714285714\n",
    "# All scores in history: [0.08972824973967813, 0.9724285714285714, 0.00416490629911104, 0.008350563158591588]\n",
    "\n",
    "\n",
    "\n",
    "# # TEST 6\n",
    "# # Build Keras model \n",
    "# model = Sequential()\n",
    "# model.add(Dense(input_dim=784, units=2000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "# model.add(Dense(1000,activation='tanh'))\n",
    "# model.add(Dense(500,activation='tanh'))\n",
    "# model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "# optimizer = SGD(lr=0.01)\n",
    "# #optimizer = Adam(lr=0.01)\n",
    "# model.compile(loss='categorical_crossentropy', \n",
    "#               optimizer=optimizer, \n",
    "#               metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# # Train\n",
    "# VERBOSE     = 0\n",
    "# EPOCHS      = 35\n",
    "\n",
    "# Training time: 1933.5 sec\n",
    "# Test loss:     0.08383155212781969\n",
    "# Test accuracy: 0.9739523809523809\n",
    "# All scores in history: [0.08383155212781969, 0.9739523809523809, 0.0038726767958377466, 0.0072450248341830005]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The hyperparameteres that gave the best accuracy was:\n",
    "```python\n",
    "# Build Keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, units=2000, activation=\"tanh\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(1000,activation='tanh'))\n",
    "model.add(Dense(500,activation='tanh'))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = SGD(lr=0.01)\n",
    "#optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=optimizer, \n",
    "              metrics=['categorical_accuracy', 'mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "# Train\n",
    "VERBOSE     = 0\n",
    "EPOCHS      = 35\n",
    "\n",
    "Training time: 1933.5 sec\n",
    "Test loss:     0.08383155212781969\n",
    "Test accuracy: 0.9739523809523809\n",
    "All scores in history: [0.08383155212781969, 0.9739523809523809, 0.0038726767958377466, 0.0072450248341830005]\n",
    "```\n",
    "And the highest accuracy was: 97\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
