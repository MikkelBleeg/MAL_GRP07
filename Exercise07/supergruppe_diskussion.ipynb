{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supergruppe diskussion\n",
    "\n",
    "## § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel (eksklusiv\"Create the Workspace\" og \"Download the Data\"), og forbered mundtlig præsentation.\n",
    "\n",
    "Lav et kort resume af de enkelte underafsnit, ca. 5 til 20 liners tekst.\n",
    "\n",
    "Husk at relater til \"The Map\":\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L07/Figs/ml_supervised_map.png\" style=\"width:400px\">\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* Look at the Big Picture\n",
    "* Get the Data (eksklusiv Create the Workspace og Download the Data),\n",
    "* Discover and Visualize the Data to Gain Insights,\n",
    "* Prepare the Data for Machine Learning Algorithms,\n",
    "* Select and Train a Model,\n",
    "* Fine-Tune Your Model,\n",
    "* Launch, Monitor, and Maintain Your System,\n",
    "* Try It Out!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume: Look at the Big Picture\n",
    "\n",
    "Setting up the frame for the mashine learning project and building a model.\n",
    "\n",
    "Background Research:\n",
    "- What is the end goal?\n",
    "- Which algorithms do we use?\n",
    "- Which preformance messures will be used to evaluate?\n",
    "- Do we already have a solution? if how should we improve it?\n",
    "\n",
    "##### Framing the problem:\n",
    "Deside on Algorithms: \n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforced learning\n",
    "\n",
    "Deside on preformance messure:\n",
    "- RMSE (messuring standart deviation)  \n",
    "- MAE\n",
    "\n",
    "<img src=\"The_Map_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Resume: Get the Data (eksklusiv Create the Workspace og Download the Data)\n",
    "\n",
    "##### Get data and create a workspace:\n",
    "- Install a editor and set up the workspace.\n",
    "- Download data and load it into into the workspace.\n",
    "\n",
    "##### Get a overview of the data:\n",
    "- What is the structure?\n",
    "- How many sampels?\n",
    "- How many and what are the Features (Atributes)?\n",
    "- Use diffrent meathods to create a overview:\n",
    "  - Use info() method to get quick information about the dataset.\n",
    "  - Use descripe() method to show a summary of the attributes.\n",
    "  - Use hist() to make a histogram over the dataset.\n",
    "    - What does the histogram tell about the data?  \n",
    "    - Look at the median.\n",
    "    - Look at the tails.\n",
    "    - Are there any apparrent problems? Should we do something with the data to work around these problems.\n",
    "\n",
    "##### Split the data into test and train sets:\n",
    "- Test set:\n",
    "  - 20% of the dataset should be split into the test set. So that the we dont test the algorithm on the same data as the one we trained it with.\n",
    "  - If random sampling: Make sure that the data in test isn't loadet into the train set by using ```random.seed(42)``` and then ```np.random.permutation```  so that it is always the same random data that is saved in test.\n",
    "  - Stratified sampling: the data is devided into hogenis subgrupes caled strata. \n",
    "  \n",
    "\n",
    "\n",
    "<img src=\"The_Map_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Resume: Discover and Visualize the Data to Gain Insights,\n",
    "More in depth work with the data, and ensure that the test set has been put aside.\n",
    "\n",
    "##### Visualizing the data:\n",
    "- Create a scatterplot of the data.\n",
    "  - Collor the data or viasualice the density in the data in another way. \n",
    "\n",
    "##### Correlations: (between the attributes)\n",
    "- Standart corralation (Linear corralation)\n",
    "  - Ranges from -1 to 1:\n",
    "  - Closer to 1 is equal to a strons positive coralation.\n",
    "  - Closer to -1 is equal to a stronger negative corralation.\n",
    "  - Close to 0 equals no linear corralation.\n",
    "- Look at other corralations:\n",
    "  - Pandas ```scatter_matrix``` function.\n",
    "  - Corraltaion Scatter plot.\n",
    "  \n",
    "##### Try diffrent attribute combinations:\n",
    "- Plot the diffrent attributes together maybe based on at pattern seen in the previouslet ccorraltions.\n",
    "\n",
    "### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "##### Create functions to prepare the data for mashine learning algorithms because:\n",
    "- You can recreate the transformationa and use the transformations on other datasets. ```dropna()```\n",
    "- You build a libary of transformation functins to reduce future ML projects. ```drop()```\n",
    "- Use them in the ML system to transform data and feed to the algorithm. ```filna()```\n",
    "\n",
    "##### Clean the data:\n",
    "- If there is missing features the algorithm cannot work with the data so this needs to be cleaned up.\n",
    "- dropna()\n",
    "- drop()\n",
    "- filna() to be uses after finding the median with .median() on the data. It fills out the missing value in the trainingset.\n",
    "\n",
    "##### Fit and train the data with you functions.\n",
    "\n",
    "##### Handling text and cartegorical attributes:\n",
    "Transformers called encoders. Because Mashine Learning algorithms work with numers and cannot wotk with labels or categories we need to transform these into numerical values, and we can do this with encoders:\n",
    "- LabelEncoder()\n",
    "- One-Hot encoding, which is a binarry encoder\n",
    "\n",
    "##### Custom transformers:\n",
    "Transformers that fits the dataset that is worked on that transform the data into data tat can be feed to the ML algorithms. these can consist of cleanup function and or functions that combines different attributes. In the transformers one can also play with combining and working on different hyper parametres.\n",
    "\n",
    "If the Custom transformers are on the same form as in ciKit learn like with base classes then the transformers will wor.\n",
    "\n",
    "##### Feature Scaling\n",
    "The attributes that we want the ML algorithm often needs to be the same scale which is why feature scaling sometimes is needet. \n",
    "\n",
    "two commom ways to scale attributes to the same scale is:\n",
    "- min-max scaling.\n",
    "- standatdization.\n",
    "  - Good with outliers.\n",
    "\n",
    "##### Transforming pipelines:\n",
    "All estimatores except for the last needs to be a transformer and must have a \n",
    "```fit_transform()``` meathod the last estimator only needs the ```fit()``` \n",
    "method. \n",
    "\n",
    "This way one can make a pipeline of the transformers with the output of one trandformer feeding data to be inputted in the next transformer. \n",
    "\n",
    "<img src=\"The_Map_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Resume: Select and Train a Model\n",
    "\n",
    "#### Training and Evaluating on the Training Set\n",
    "\n",
    "It's important to train your data, so you'll get the proper prediction. If the data is not trained, the code will still work, but the prediction will be way off.\n",
    "\n",
    "#### Better Evaluation Using Cross-Validation\n",
    "\n",
    "It's possible to use the train_test_split function to split the training into a smaller training set and a validation set. Then you'll train your models against the smaller set and evaluate them against the validation set.\n",
    "\n",
    "For this you can use Scikit-Learn's cross-validation feature. Which randomly splits the training set into 10 subsets called folds. It will then train and evaluate the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds.\n",
    "\n",
    "An issue with this is, that the Decision Tree model can overfit which makes the result much worse.\n",
    "\n",
    "A way to deal with this is to use the RandomForestRegressor. This function will train many Decision Trees on random subsets of the features, then averaging out their predictions.\n",
    "\n",
    "It may still overfit, a way to completely solve overfitting, is to simplify the model, constrain it or get a lot more training data.\n",
    "\n",
    "<img src=\"The_Map_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Resume: Fine-Tune Your Model\n",
    "Assume we have a shortlist of promising models, that now needs fine tuning.\n",
    "A few approaches listed:\n",
    "\n",
    "#### Grid Search\n",
    "- HyperParameters search - scikit-Learn's GridSearchCV\n",
    "  - sklearn's function to search with hyperparameters Which needs hyperparameters and values to try out\n",
    "  - Evaluate all possible combis using Cross-validation.\n",
    "  - Uses N_estimators and Max_features an\n",
    "- Also possible to use Best_estimator\n",
    "- Default refit=true, will find the best and refit\n",
    "\n",
    "#### Randomized search\n",
    "- Grid search is good when few combi but random might be better when search space is large\n",
    "- Instead of trying all combi's\n",
    "  - It evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration\n",
    "  - Say 1000 iterations, the approach will explore 1000 different values for each hyperparameter instead of few values\n",
    "#### Ensemble method\n",
    "- Combines models that perform the best\n",
    "\n",
    "#### analyze the best and their errors\n",
    "- RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions\n",
    "- With this new information you can try to drop less useful features\n",
    "\n",
    "#### Evaluate your system on the test\n",
    "- When a suffeciently well perfoming system is found. It is time to use final model on test set\n",
    "- Use transform() and not fit_transform()\n",
    "- Same procedure as fit_transform after estimation\n",
    "- Performance slightly worse than cross-validation(if a lot of hyperparameter tuning was done)\n",
    "- Performs worse on unknown datasets\n",
    "\n",
    "\n",
    "#### Prelaunch phase\n",
    "- Present solution\n",
    "- Highlight: \n",
    "  - what has been learned\n",
    "  - what did/did not work\n",
    "  - assumptions made in the process\n",
    "  - system limitation\n",
    "- Make a nice presentation which is easy to remember\n",
    "\n",
    "<img src=\"The_Map_4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"The_Map_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"The_Map_6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### Resume: Launch, Monitor, and Maintain Your System\n",
    "- Get solution ready for production\n",
    "- Write Monitor code, which can trigger alerts. \n",
    "- Check for system degradation\n",
    "- Human evalutation is sometimes need\n",
    "- Occasionally, retrain system with fresh data \n",
    "  - Also check data quality, when training/retraining\n",
    "- Try to make training/retraining and automate process\n",
    "- If it is an online system, document the state occasionally and keep version control\n",
    "\n",
    "\n",
    "<img src=\"The_Map_7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Resume: Try It Out!.\n",
    "\n",
    "This chapter is about understanding the importance of the work you put in the data preparation step, building monitoring tools, setting up human evaluation pipelines and automating regular model training. A good way to perform in machinelearning, is to learn a few algorithms well, instead of always trying to find advanced and new algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
