{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0301| CEF, initial.\n",
    "2018-0305| CEF, updated.\n",
    "2018-0306| CEF, updated and spell checked.\n",
    "2018-0306| CEF, major overhaul of functions.\n",
    "2018-0306| CEF, fixed problem with MNIST load and Keras.\n",
    "2018-0307| CEF, modified report functions and changed Qc+d.\n",
    "2018-0311| CEF, updated Qd.\n",
    "2018-0312| CEF, added grid and random search figs and added bullets to Qd.\n",
    "2018-0313| CEF, fixed SVC and gamma issue, and changed dataload to be in fetchmode (non-keras).\n",
    "2019-1015| CEF, updated for ITMAL E19\n",
    "2019-1019| CEF, minor text update.\n",
    "2019-1023| CEF, changed demo model i Qd) from MLPClassifier to SVC.\n",
    "\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparametes___---in contrast to _normal_ model parameters, say the weights, $\\mathbf w$, for an `SGD` model.\n",
    "\n",
    "An example could be the python constructor for the support-vector classifier `sklearn.svm.SVC`, with, say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`? \n",
    "\n",
    "```python\n",
    "class sklearn.svm.SVC(\n",
    "    C=1.0, \n",
    "    kernel=’rbf’, \n",
    "    degree=3,\n",
    "    gamma=’auto_deprecated’, \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape=’ovr’, \n",
    "    random_state=None\n",
    "  )\n",
    "```  \n",
    "\n",
    "The default values might be sensible a general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result. \n",
    "\n",
    "To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n",
    "\n",
    "An alternative is just to __brute-force__ a search of interesting hyperparameters, an choosing the 'best' parameters according to a fit-predict and some performance metric, say 'f1'. \n",
    "\n",
    "Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n",
    "\n",
    "```python\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "```\n",
    "and fire up a full (grid) search on this hyperparameter set, that will try out all combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L08/Figs/gridsearch.png\" style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center> \n",
    "    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n",
    "</em></small>\n",
    "\n",
    "The demo code below sets up some of our well known 'hello-world' data and run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n",
    "\n",
    "Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "There are two code cells below: 1:) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into to much detail).\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? \n",
    "\n",
    "NOTICE: you need the dataloader module from `libitmal`, clone \n",
    "```\n",
    "> git clone https://cfrigaard@bitbucket.org/cfrigaard/itmal\n",
    "\n",
    "```\n",
    "or pull the GIT repository to get the latest version, and put `libitmal` into the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            r=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                t = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(r)>0:\n",
    "                    r += ','\n",
    "                r += f'{key}={t}{value}{t}'  \n",
    "            return r            \n",
    "        try:\n",
    "            p = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + p + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(fetchmode=True)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}'\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "hello\n",
      "SEARCH TIME: 1.35 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285714\n",
      "\tbest index=0\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.971 (+/-0.048) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.695 (+/-0.031) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.952 (+/-0.084) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.914 (+/-0.111) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK\n",
      "Iris takes about 1.15 Sec\n",
      "Moon takes about 2.02 Sec\n",
      "mnist takes about x.xx Sec\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris') # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "print(\"hello\")\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')\n",
    "\n",
    "print('Iris takes about 1.15 Sec')\n",
    "print('Moon takes about 2.02 Sec')\n",
    "print('mnist takes about x.xx Sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa Answer\n",
    "\n",
    "GridSearchCV is an exhaustive search over specified parameter values for an estimator. \n",
    "It works by letting the user combine an estimator with a grid search preabmle to tune hyper-parameters. It evaluates all possible combinations with cross-validation. It can implement a refit model to the best estimator if the parameter is set to true, aswell as returning the score for the estimator.\n",
    "\n",
    "The search parameter set is created by using the model created with svm.SVC.\n",
    "The scoring is a parameter used to evaluate the prediction on the test set. In this case f1_micro score parameter is used. \n",
    "\n",
    "n_jobs is the amount of jobs running in parallel, where -1 means using all the processors. By using the f1_micro, the search evaluates the F1_score by calculating the metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "\n",
    "does a grid search and refits the model to the best estimator. Afterwards it is trained on a new training set and tested on a test set, where a report log is printed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SDG` in the search-space before it begins to take considerable compute time doing the full grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "hello\n",
      "SEARCH TIME: 1.42 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.001, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\tbest 'f1_micro' score=0.9714285714285714\n",
      "\tbest index=14\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='adaptive', loss='hinge',\n",
      "              max_iter=100, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.790 (+/-0.236) for {'alpha': 0.001, 'learning_rate': 'constant', 'max_iter': 1}\n",
      "\t[ 1]: 0.771 (+/-0.238) for {'alpha': 0.001, 'learning_rate': 'constant', 'max_iter': 10}\n",
      "\t[ 2]: 0.705 (+/-0.341) for {'alpha': 0.001, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[ 3]: 0.657 (+/-0.348) for {'alpha': 0.001, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[ 4]: 0.676 (+/-0.071) for {'alpha': 0.001, 'learning_rate': 'optimal', 'max_iter': 1}\n",
      "\t[ 5]: 0.771 (+/-0.161) for {'alpha': 0.001, 'learning_rate': 'optimal', 'max_iter': 10}\n",
      "\t[ 6]: 0.838 (+/-0.283) for {'alpha': 0.001, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[ 7]: 0.876 (+/-0.183) for {'alpha': 0.001, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[ 8]: 0.695 (+/-0.031) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'max_iter': 1}\n",
      "\t[ 9]: 0.781 (+/-0.183) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'max_iter': 10}\n",
      "\t[10]: 0.867 (+/-0.082) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[11]: 0.905 (+/-0.081) for {'alpha': 0.001, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[12]: 0.695 (+/-0.031) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'max_iter': 1}\n",
      "\t[13]: 0.810 (+/-0.445) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'max_iter': 10}\n",
      "\t[14]: 0.971 (+/-0.048) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\t[15]: 0.962 (+/-0.070) for {'alpha': 0.001, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\t[16]: 0.676 (+/-0.062) for {'alpha': 0.01, 'learning_rate': 'constant', 'max_iter': 1}\n",
      "\t[17]: 0.667 (+/-0.084) for {'alpha': 0.01, 'learning_rate': 'constant', 'max_iter': 10}\n",
      "\t[18]: 0.695 (+/-0.031) for {'alpha': 0.01, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[19]: 0.819 (+/-0.149) for {'alpha': 0.01, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[20]: 0.667 (+/-0.070) for {'alpha': 0.01, 'learning_rate': 'optimal', 'max_iter': 1}\n",
      "\t[21]: 0.790 (+/-0.251) for {'alpha': 0.01, 'learning_rate': 'optimal', 'max_iter': 10}\n",
      "\t[22]: 0.848 (+/-0.179) for {'alpha': 0.01, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[23]: 0.895 (+/-0.105) for {'alpha': 0.01, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[24]: 0.695 (+/-0.031) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'max_iter': 1}\n",
      "\t[25]: 0.838 (+/-0.211) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'max_iter': 10}\n",
      "\t[26]: 0.743 (+/-0.129) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[27]: 0.790 (+/-0.182) for {'alpha': 0.01, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[28]: 0.600 (+/-0.318) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'max_iter': 1}\n",
      "\t[29]: 0.790 (+/-0.250) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'max_iter': 10}\n",
      "\t[30]: 0.943 (+/-0.136) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\t[31]: 0.971 (+/-0.075) for {'alpha': 0.01, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\t[32]: 0.590 (+/-0.292) for {'alpha': 0.1, 'learning_rate': 'constant', 'max_iter': 1}\n",
      "\t[33]: 0.676 (+/-0.092) for {'alpha': 0.1, 'learning_rate': 'constant', 'max_iter': 10}\n",
      "\t[34]: 0.495 (+/-0.280) for {'alpha': 0.1, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[35]: 0.533 (+/-0.283) for {'alpha': 0.1, 'learning_rate': 'constant', 'max_iter': 100}\n",
      "\t[36]: 0.629 (+/-0.117) for {'alpha': 0.1, 'learning_rate': 'optimal', 'max_iter': 1}\n",
      "\t[37]: 0.819 (+/-0.319) for {'alpha': 0.1, 'learning_rate': 'optimal', 'max_iter': 10}\n",
      "\t[38]: 0.810 (+/-0.205) for {'alpha': 0.1, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[39]: 0.762 (+/-0.193) for {'alpha': 0.1, 'learning_rate': 'optimal', 'max_iter': 100}\n",
      "\t[40]: 0.657 (+/-0.099) for {'alpha': 0.1, 'learning_rate': 'invscaling', 'max_iter': 1}\n",
      "\t[41]: 0.762 (+/-0.184) for {'alpha': 0.1, 'learning_rate': 'invscaling', 'max_iter': 10}\n",
      "\t[42]: 0.695 (+/-0.031) for {'alpha': 0.1, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[43]: 0.695 (+/-0.031) for {'alpha': 0.1, 'learning_rate': 'invscaling', 'max_iter': 100}\n",
      "\t[44]: 0.562 (+/-0.295) for {'alpha': 0.1, 'learning_rate': 'adaptive', 'max_iter': 1}\n",
      "\t[45]: 0.714 (+/-0.143) for {'alpha': 0.1, 'learning_rate': 'adaptive', 'max_iter': 10}\n",
      "\t[46]: 0.848 (+/-0.078) for {'alpha': 0.1, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\t[47]: 0.829 (+/-0.078) for {'alpha': 0.1, 'learning_rate': 'adaptive', 'max_iter': 100}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='adaptive', loss='hinge',\n",
      "              max_iter=100, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SGDClassifier(alpha=0.001,learning_rate='adaptive',max_iter=100)\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qb, test with SGDClassifier\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris') # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "\n",
    "model = SGDClassifier(eta0=0.1) \n",
    "tuning_parameters = {\n",
    "    'alpha' : [0.001, 0.01, 0.1],\n",
    "    'max_iter': [1, 10, 100, 100],\n",
    "    'learning_rate': ('constant','optimal','invscaling','adaptive')\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "print(\"hello\")\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/E19_itmal/L08/Figs/randomsearch.png\" style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use the same parameters for the random search, but add and investigate the new `n_iter` parameter\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    random_state=42, \n",
    "    n_iter=20, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1, \n",
    "    iid=True)\n",
    "```\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minute, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc, try with randomized search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris') # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "# Run RandomizedSearchCV for the model\n",
    "start = time()\n",
    "random_tuned = RandomizedSearchCV(model, tuning_parameters, random_state=42, n_iter=20, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "random_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(random_tuned , X_test, y_test, t)\n",
    "print('OK')\n",
    "print('Iris takes about 1.08 Sec')\n",
    "print('Moon takes about 1.92 Sec')\n",
    "print('mnist takes about x.xx Sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd MNIST Search Quest II\n",
    "\n",
    "Finally, we create yet a search-quest competition: who can find the best model+hyperparameters for MNIST dataset?\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the _tiny-data_ iris: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for an exhaustive grid search, or a faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---except Neural Networks!. Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale you input data for some models to perform better (neural networks in particular).\n",
    "\n",
    "* DO NOT USE any Neural Network models, including Keras or Tensorflow models...not yet, and there are too many examples on the net to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=iris, score=0.97143, model=SVC(C=1, kernel='linear')\n",
    "```\n",
    "Now, check if your score (for MNIST) is better that the currently best score on Blackboard: \"L07: Optimization and searching\" | \"Search Quest for MNIST\"\n",
    "\n",
    "> https://blackboard.au.dk/webapps/blackboard/content/listContentEditable.jsp?content_id=_2302642_1&course_id=_131051_1&mode=reset\n",
    "\n",
    "and paste your best model into the message box, like\n",
    "```\n",
    "best(Grp99): dat=mnist, score=0.47090, model=SVC(C=1, kernel='linear')\n",
    "```\n",
    "Remember to provide a ITMAL group name manually, so we can identify a winnner: the 1.st price is yet a cake! \n",
    "\n",
    "For the handin, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd..(in code and text..)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
